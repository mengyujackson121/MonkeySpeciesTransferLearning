{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b4b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision, torch\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1021fa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.9.0\n",
      "Torchvision Version:  0.10.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from pytorch_transfer_learn import initialize_model, get_data_sets, train_model, set_training_mode, display_confusion_matrix\n",
    "from keras_transfer_learn import load_data_sets, load_model\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8d7d3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mengyujackson121'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e00393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"/mnt/e/flatiron/project5/image\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"squeezenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 11\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef0ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0191 Acc: 0.6818\n",
      "val Loss: 0.2705 Acc: 0.9025\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4637 Acc: 0.8497\n",
      "val Loss: 0.2141 Acc: 0.9364\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4179 Acc: 0.8698\n",
      "val Loss: 0.1943 Acc: 0.9386\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4118 Acc: 0.8662\n",
      "val Loss: 0.1842 Acc: 0.9386\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2863 Acc: 0.9087\n",
      "val Loss: 0.2345 Acc: 0.9280\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3779 Acc: 0.8922\n",
      "val Loss: 0.2721 Acc: 0.9322\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3245 Acc: 0.8904\n",
      "val Loss: 0.1068 Acc: 0.9640\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3585 Acc: 0.8851\n",
      "val Loss: 0.1864 Acc: 0.9407\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2838 Acc: 0.9146\n",
      "val Loss: 0.1834 Acc: 0.9513\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2829 Acc: 0.8992\n",
      "val Loss: 0.2117 Acc: 0.9343\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2863 Acc: 0.9093\n",
      "val Loss: 0.1211 Acc: 0.9682\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2875 Acc: 0.9034\n",
      "val Loss: 0.2081 Acc: 0.9407\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2803 Acc: 0.9087\n",
      "val Loss: 0.1519 Acc: 0.9492\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2845 Acc: 0.9022\n",
      "val Loss: 0.1278 Acc: 0.9576\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengyujackson121/anaconda3/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2602 Acc: 0.9122\n",
      "val Loss: 0.1005 Acc: 0.9682\n",
      "\n",
      "Training complete in 10m 35s\n",
      "Best val Acc: 0.968220\n"
     ]
    }
   ],
   "source": [
    "for model_name in [\"resnet\", \"alexnet\", \"vgg\", \"squeezenet\", \"densenet\", \"inception\"]:\n",
    "    # Initialize the model for this run\n",
    "    model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "    # Print the model we just instantiated\n",
    "    print(model_ft)\n",
    "    dataloaders_dict = get_data_sets(input_size, data_dir, batch_size)\n",
    "    optimizer_ft = set_training_mode(model_ft, feature_extract)\n",
    "\n",
    "    # Detect if we have a GPU available\n",
    "    # Setup the loss fxn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and evaluate\n",
    "    model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "    display_confusion_matrix(model_ft, device, dataloaders_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0bded83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1697 files belonging to 11 classes.\n",
      "Found 472 files belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "img_height = 150\n",
    "img_width = 150\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63f81a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://keras.io/guides/transfer_learning/\n",
    "\n",
    "train_ds, val_ds = load_data_sets(data_dir, img_height, img_width, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "688cde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5f7c1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['monkey_labels.txt', 'oregon_wildlife', 'train', 'val', 'wildlife.h5']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/mnt/e/flatiron/project5/image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f28024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "normalization_4 (Normalizati (None, 150, 150, 3)       7         \n",
      "_________________________________________________________________\n",
      "xception (Functional)        (None, 5, 5, 2048)        20861480  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_6 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 11)                22539     \n",
      "=================================================================\n",
      "Total params: 20,884,026\n",
      "Trainable params: 22,539\n",
      "Non-trainable params: 20,861,487\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"xception\")\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1b162d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "54/54 [==============================] - 127s 2s/step - loss: 1.4291 - categorical_accuracy: 0.5624 - val_loss: 0.2923 - val_categorical_accuracy: 0.9174\n",
      "Epoch 2/20\n",
      "54/54 [==============================] - 114s 2s/step - loss: 0.2669 - categorical_accuracy: 0.9209 - val_loss: 0.2464 - val_categorical_accuracy: 0.9301\n",
      "Epoch 3/20\n",
      "54/54 [==============================] - 111s 2s/step - loss: 0.1669 - categorical_accuracy: 0.9563 - val_loss: 0.2178 - val_categorical_accuracy: 0.9386\n",
      "Epoch 4/20\n",
      "54/54 [==============================] - 115s 2s/step - loss: 0.1199 - categorical_accuracy: 0.9671 - val_loss: 0.2056 - val_categorical_accuracy: 0.9449\n",
      "Epoch 5/20\n",
      "54/54 [==============================] - 113s 2s/step - loss: 0.0905 - categorical_accuracy: 0.9805 - val_loss: 0.2091 - val_categorical_accuracy: 0.9428\n",
      "Epoch 6/20\n",
      "54/54 [==============================] - 96s 2s/step - loss: 0.0722 - categorical_accuracy: 0.9877 - val_loss: 0.2031 - val_categorical_accuracy: 0.9407\n",
      "Epoch 7/20\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0525 - categorical_accuracy: 0.9931 - val_loss: 0.2016 - val_categorical_accuracy: 0.9364\n",
      "Epoch 8/20\n",
      "54/54 [==============================] - 82s 1s/step - loss: 0.0443 - categorical_accuracy: 0.9962 - val_loss: 0.1956 - val_categorical_accuracy: 0.9386\n",
      "Epoch 9/20\n",
      "54/54 [==============================] - 85s 2s/step - loss: 0.0414 - categorical_accuracy: 0.9950 - val_loss: 0.1964 - val_categorical_accuracy: 0.9449\n",
      "Epoch 10/20\n",
      "54/54 [==============================] - 82s 1s/step - loss: 0.0294 - categorical_accuracy: 1.0000 - val_loss: 0.1959 - val_categorical_accuracy: 0.9364\n",
      "Epoch 11/20\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0305 - categorical_accuracy: 0.9998 - val_loss: 0.1940 - val_categorical_accuracy: 0.9343\n",
      "Epoch 12/20\n",
      "54/54 [==============================] - 82s 1s/step - loss: 0.0262 - categorical_accuracy: 0.9991 - val_loss: 0.1984 - val_categorical_accuracy: 0.9364\n",
      "Epoch 13/20\n",
      "54/54 [==============================] - 82s 1s/step - loss: 0.0243 - categorical_accuracy: 0.9989 - val_loss: 0.1981 - val_categorical_accuracy: 0.9428\n",
      "Epoch 14/20\n",
      "54/54 [==============================] - 83s 1s/step - loss: 0.0223 - categorical_accuracy: 0.9984 - val_loss: 0.2045 - val_categorical_accuracy: 0.9407\n",
      "Epoch 15/20\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0167 - categorical_accuracy: 1.0000 - val_loss: 0.1981 - val_categorical_accuracy: 0.9364\n",
      "Epoch 16/20\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0155 - categorical_accuracy: 1.0000 - val_loss: 0.2005 - val_categorical_accuracy: 0.9343\n",
      "Epoch 17/20\n",
      "54/54 [==============================] - 82s 1s/step - loss: 0.0149 - categorical_accuracy: 1.0000 - val_loss: 0.2004 - val_categorical_accuracy: 0.9407\n",
      "Epoch 18/20\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0119 - categorical_accuracy: 1.0000 - val_loss: 0.2042 - val_categorical_accuracy: 0.9364\n",
      "Epoch 19/20\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0113 - categorical_accuracy: 1.0000 - val_loss: 0.2052 - val_categorical_accuracy: 0.9428\n",
      "Epoch 20/20\n",
      "54/54 [==============================] - 78s 1s/step - loss: 0.0109 - categorical_accuracy: 1.0000 - val_loss: 0.2015 - val_categorical_accuracy: 0.9407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f41f8ba8610>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(train_ds, epochs=num_epochs, validation_data=val_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0a21979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 150, 150, 3), (None, 3)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15faca06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}